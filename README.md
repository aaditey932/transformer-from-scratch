# ğŸš€ Transformer Model from Scratch

This repository contains a **from-scratch implementation** of the **Transformer architecture** using **NumPy**. The model is inspired by the paper ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) and implements **multi-head self-attention, positional encoding, and feed-forward layers**.

---

## ğŸ“Œ Features

âœ”ï¸ **Built from Scratch** â€“ No deep learning frameworks (except NumPy)  
âœ”ï¸ **Multi-Head Self-Attention** â€“ Implements scaled dot-product attention  
âœ”ï¸ **Positional Encoding** â€“ Adds positional information to embeddings  
âœ”ï¸ **Feed-Forward Networks** â€“ Fully connected layers with activation  
âœ”ï¸ **Encoder-Decoder Architecture** â€“ Implements a full Transformer  
âœ”ï¸ **Custom Masking Mechanisms** â€“ Supports padding & look-ahead masking  

---

## âš¡ Installation

Clone the repository:

```bash
git clone https://github.com/yourusername/transformer-from-scratch.git
cd transformer-from-scratch
