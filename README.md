# 🚀 Transformer Model from Scratch

This repository contains a **from-scratch implementation** of the **Transformer architecture** using **NumPy**. The model is inspired by the paper ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) and implements **multi-head self-attention, positional encoding, and feed-forward layers**.

---

## 📌 Features

✔️ **Built from Scratch** – No deep learning frameworks (except NumPy)  
✔️ **Multi-Head Self-Attention** – Implements scaled dot-product attention  
✔️ **Positional Encoding** – Adds positional information to embeddings  
✔️ **Feed-Forward Networks** – Fully connected layers with activation  
✔️ **Encoder-Decoder Architecture** – Implements a full Transformer  
✔️ **Custom Masking Mechanisms** – Supports padding & look-ahead masking  

---

## ⚡ Installation

Clone the repository:

```bash
git clone https://github.com/yourusername/transformer-from-scratch.git
cd transformer-from-scratch
